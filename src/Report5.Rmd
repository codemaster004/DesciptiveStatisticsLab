---
title: "Regression Analysis"
author: "Your Name"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
download.file("https://github.com/kflisikowski/ds/blob/master/Real%20estate.csv?raw=true", destfile ="real_estates.csv",mode="wb")
```

## Introduction

Our aim is to predict house values. Before we begin to do any analysis, we should always check whether the dataset has missing value or not, we do so by typing:

```{r dataset, warning=FALSE}
taiwan_real_estate <- read.csv("real_estates.csv",row.names=1)
attach(taiwan_real_estate)
any(is.na(taiwan_real_estate))
```

Let's take a look at structure of the data set:

```{r dataset2, warning=TRUE}
glimpse(taiwan_real_estate)
```

Let's simplify variables' names:

```{r}
taiwan_real_estate <- taiwan_real_estate %>%
rename(house_age_years = house.age, price_twd_msq = house.price.of.unit.area,
       n_convenience = number.of.convenience.stores, 
       dist_to_mrt_m = distance.to.the.nearest.MRT.station)
```

We can also perform binning for "house_age_years":

```{r}
#perform binning with specific number of bins
taiwan_real_estate<-taiwan_real_estate %>% mutate(house_age_cat = cut(house_age_years, breaks=c(0,15,30,45),include.lowest = T,
                                                                        right = F))
```

## Descriptive Statistics

Prepare a heatmap with correlation coefficients on it:

```{r}
cor(taiwan_real_estate$house_age_years, taiwan_real_estate$price_twd_msq)
cor(taiwan_real_estate$dist_to_mrt_m, taiwan_real_estate$price_twd_msq)
cor(taiwan_real_estate$n_convenience, taiwan_real_estate$price_twd_msq)
cor(taiwan_real_estate$latitude, taiwan_real_estate$price_twd_msq)
cor(taiwan_real_estate$longitude, taiwan_real_estate$price_twd_msq)

library(corrplot)
M = cor(taiwan_real_estate[,1:6])
corrplot(M, method='number')
```


Draw a scatter plot of n_convenience vs. price_twd_msq:

```{r}
ggplot(data=taiwan_real_estate, aes(x=dist_to_mrt_m, y=price_twd_msq)) +
  geom_point() +
  geom_smooth(method = lm)
```

Draw a scatter plot of house_age_years vs. price_twd_msq:

```{r}
ggplot(data=taiwan_real_estate, aes(x=house_age_years, y=price_twd_msq)) +
  geom_point() +
  geom_smooth(method = lm)
```

Draw a scatter plot of distance to nearest MRT station vs. price_twd_msq:

```{r}
ggplot(data=taiwan_real_estate, aes(x=dist_to_mrt_m, y=price_twd_msq)) +
  geom_point() +
  geom_smooth(method = lm)
```

Plot a histogram of price_twd_msq with 10 bins, facet the plot so each house age group gets its own panel:

```{r}
ggplot(taiwan_real_estate, aes(price_twd_msq)) +
  geom_histogram(bins=10)
```


Looking at outliers for price
```{r}
ggplot(taiwan_real_estate, aes(price_twd_msq)) +
  geom_boxplot()

```

Summarize to calculate the mean, sd, median etc. house price/area by house age:

```{r}
taiwan_real_estate %>% 
  group_by(house_age_cat) %>% 
  summarise(mean=mean(price_twd_msq), median=median(price_twd_msq), n=n())
```

## Simple model

Run a linear regression of price_twd_msq vs. best, but only 1 predictor:

```{r}
model1 = lm(data=taiwan_real_estate, formula=price_twd_msq~dist_to_mrt_m)
model1
```

We start by displaying the statistical summary of the model using the R function summary():

```{r}
summary(model1)
```


You can access lots of different aspects of the regression object. To see what’s inside, use names():

```{r}
names(model1)
```

What do they mean?

coefficients- They shows the relationship between the predictors and the response variable. 

residuals- They indicate how well the model fits the data.

effects- The orthogonal effects of the model terms. These are used internally by the model.

rank- It represents the number of linearly independent columns in the model matrix.

fitted.values- These are the values of the response variable as predicted by the model.

assign- This is used internally to map the coefficients to the corresponding terms in the model.

qr- This is used internally for computing the model.

df.residual- It is the number of observations minus the number of fitted coefficients (including the intercept).

xlevels- This is relevant if the model includes categorical predictors.

call- It shows the function call that created the model.

terms- Formula of models.

model- the data frame containing the variables used in the model.

Discuss model accuracy:
R-squared (R²):
R-squared is the proportion of the variance in the dependent variable that is predictable from the independent variable(s). An R-squared value of 0.5503 means that approximately 55.03% of accuracy. Higher R-squared values indicate a better fit of the model to the data.

Adjusted R-squared:
 Adjusted R-squared adjusts the R-squared value for the number of predictors in the model, providing a more accurate measure when comparing models with a different number of predictors. This value is particularly useful when dealing with multiple regression models. It penalizes the addition of irrelevant predictors, thus providing a more reliable measure of model performance.

p-value:
The p-value for the predictor variable tests the null hypothesis that the coefficient of the predictor is equal to zero (no effect).A p-value less than 0.05 typically indicates that the predictor variable is statistically significant, if the p-value for the distance to the MRT station is less than 0.05, it suggests that the distance to the MRT station has a significant effect on the house prices.

Residual Standard Error (RSE):
RSE measures the standard deviation of the residuals (the differences between observed and predicted values). A lower RSE indicates a better fit of the model. It gives an absolute measure of how well the model predicts the response variable.

F-statistic:
The F-statistic tests the overall significance of the model. It compares the model with no predictors to the model with the predictors specified. A higher F-statistic value indicates that the model provides a better fit to the data than a model with no predictors. The associated p-value helps determine the statistical significance of the F-statistic. A p-value less than 0.05 suggests that the model is statistically significant.

Model diagnostics:

```{r}
par(mfrow = c(2, 2))
plot(model1)
```

The four plots show...

Residuals vs Fitted: By this plot we can checks the linearity assumptions. In our case if the residuals are randomly scattered nearby the horizontal line at zero, our assumption is proven.

Normal Q-Q: This plot shows the normality of residuals. In our example if the residuals fall with or along the straight line, it means that the residuals are normally distributed.

Scale-Location (Spread-Location): This plot checks homoscedasticity (constant variance of residuals). A horizontal line with equally spread points suggests homoscedasticity. A funnel shape indicates heteroscedasticity.

Residuals vs Leverage: This function identifies influential cases that might overly affect the model. Look for points outside the Cook’s distance lines, indicating high leverage points or influential observations.

Create the diagnostic plots using ggfortify:

```{r}
library(ggfortify)
autoplot(model1)
```

Outliers and High Leverage Points

To identify potential outliers and high leverage points, we use the leverage plot:

```{r}
plot(model1, which = 5)
```


Influential values:

```{r}
# Cook's distance
plot(model1, which = 4)
```

or just plot all of diagnostic plots together:

```{r}
autoplot(model1, which = 1:6, label.size = 3)
```

Discussion:

Showen plots provide critical insights into the meaningfull of the regression model.

Residuals vs Fitted Plot:
In our case the residuals are plotted nearbyu the horizontal line at zero. It is great, because linearity of our model is proven and it means that the relationship between the predictors and the response variable is likely linear. Of course some points are totally out, but the most important is that the density neaby zero line is big.

Normal Q-Q Plot:
The points mostly lie along the reference line, with some deviations at the tails. It means that. bThe residuals are approximately normally distributed, which is important for the validity of the hypothesis tests and confidence intervals. Minor deviations at the tails may indicate slight non-normality, but it is not severe.

Scale-Location Plot:
The residuals seems to have a constant spread across the range of fitted values.It suggests homoscedasticity, indicating that the variance of the residuals is constant across different levels of the fitted values. This is a key assumption of linear regression.

Cook's Distance Plot:
There are a few points with Cook’s distance values significantly higher than others. These points are influential and may have a disproportionate impact on the regression model. They should be examined individually to determine whether they are valid data points or potential outliers.
Residuals vs Leverage Plot:

## Multiple Regression Model

to be continued next week...
